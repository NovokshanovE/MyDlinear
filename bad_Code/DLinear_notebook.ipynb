{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sympy import N, false\n",
    "# from Dlinear_v2.MyDLinear import DLinear\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alive_progress import alive_bar\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLinear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DLinearModel, self).__init__()\n",
    "        self.linear_seasonal = nn.Linear(input_size, output_size)\n",
    "        self.linear_trend = nn.Linear(input_size, output_size)\n",
    "        self.decomposition = DecompositionLayer(input_size)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, context):\n",
    "        seasonal, trend = self.decomposition(context)\n",
    "        #print(seasonal, trend)\n",
    "        seasonal_output = self.linear_seasonal(seasonal.reshape(1, 1, -1))\n",
    "        trend_output = self.linear_trend(trend.reshape(1, 1, -1))\n",
    "        \n",
    "        return seasonal_output + trend_output\n",
    "    \n",
    "    def decompos(self, x):\n",
    "        seasonal, trend = self.decomposition(x)\n",
    "        return seasonal, trend\n",
    "    \n",
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(OneLayerModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "        \n",
    "        # self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, context):\n",
    "        # seasonal, trend = self.decomposition(context)\n",
    "        #print(seasonal, trend)\n",
    "        # seasonal_output = self.linear_seasonal(seasonal.reshape(1, 1, -1))\n",
    "        output = self.linear(context.reshape(1, 1, -1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def decompos(self, x):\n",
    "        seasonal, trend = self.decomposition(x)\n",
    "        return seasonal, trend\n",
    "    \n",
    "class DLinearModelSTL(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DLinearModelSTL, self).__init__()\n",
    "        self.linear_seasonal = nn.Linear(input_size, output_size)\n",
    "        self.linear_trend = nn.Linear(input_size, output_size)\n",
    "        self.linear_resid = nn.Linear(input_size, output_size)\n",
    "        # self.decomposition = DecompositionLayer(input_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, context):\n",
    "        seasonal, trend, resid = self.decomposition(context)\n",
    "        #print(seasonal, trend)\n",
    "        seasonal_output = self.linear_seasonal(seasonal.reshape(1, 1, -1))\n",
    "        trend_output = self.linear_trend(trend.reshape(1, 1, -1))\n",
    "        resid_output = self.linear_resid(resid.reshape(1, 1, -1))\n",
    "        \n",
    "        return seasonal_output + trend_output + resid_output\n",
    "    \n",
    "    def decomposition(self, x):\n",
    "        stl = STL(pd.Series(x.view(-1).tolist()), period=10)\n",
    "        res = stl.fit()\n",
    "        seasonal, trend, resid = torch.tensor(res.seasonal.tolist(), dtype=torch.float32).view(1, -1, 1),\\\n",
    "        torch.tensor(res.trend.tolist(), dtype=torch.float32).view(1, -1, 1),\\\n",
    "        torch.tensor(res.resid.tolist(), dtype=torch.float32).view(1, -1, 1)\n",
    "        return seasonal, trend, resid\n",
    "\n",
    "class MyDataset(TensorDataset):\n",
    "    def __init__(self, data, window, output):\n",
    "        self.data = data\n",
    "        self.window = window\n",
    "        self.output = output\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y = self.data[index+self.window:index+self.window+self.output]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window - self.output\n",
    "    \n",
    "class DecompositionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns the trend and the seasonal parts of the time series.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0) # moving average \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Input shape: Batch x Time x EMBED_DIM\"\"\"\n",
    "        # padding on the both ends of time series\n",
    "        num_of_pads = (self.kernel_size) // 2\n",
    "        front = x[:, 0:1, :].repeat(1, num_of_pads, 1)\n",
    "        end = x[:, -1:, :].repeat(1, num_of_pads, 1)\n",
    "        x_padded = torch.cat([front, x, end], dim=1)\n",
    "        # print(x)\n",
    "        # result_add = seasonal_decompose(x_padded, model='additive')\n",
    "        \n",
    "        # calculate the trend and seasonal part of the series\n",
    "        x_trend = self.avg(x_padded.permute(0, 2, 1))[:,:,:-1].permute(0, 2, 1)\n",
    "        # print(\"Delta trend:\", x_trend - result_add.trend)\n",
    "        #print(x_trend.shape)\n",
    "        x_seasonal = x - x_trend\n",
    "        return x_seasonal, x_trend\n",
    "    \n",
    "class DLinear:\n",
    "    def __init__(self, data_set = 1000, input_size = 100, output_size = 100, learning_rate = 0.00001, step = 1, data_size = 3000, column_name = \"HUFL\", dataset_name = 'dataset'):\n",
    "        torch.set_num_threads(20)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print('Using device:', device)\n",
    "        print()\n",
    "\n",
    "        #Additional Info when using cuda\n",
    "        if device.type == 'cuda':\n",
    "            print(torch.cuda.get_device_name(0))\n",
    "            print('Memory Usage:')\n",
    "            print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "            print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "        self.input_size = input_size\n",
    "        self.pred = self.input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.output_size = output_size\n",
    "        self.data_size  = data_size\n",
    "        self.step = step\n",
    "\n",
    "        self.data_set = data_set\n",
    "        self.column_name = column_name\n",
    "        self.model_name = f\"dlinear({datetime.datetime.now()})_{dataset_name}_{self.column_name}_input{self.input_size}_output{self.output_size}\"\n",
    "        self.model = None\n",
    "    def train_model(self, model, dataloader, criterion, optimizer, num_epochs=100):\n",
    "        with alive_bar(num_epochs) as bar:\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                bar()\n",
    "                \n",
    "                for X, Y in dataloader:\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    #print(X, Y)\n",
    "                    output = model.forward(X).view(1, -1, 1)\n",
    "                    #print(torch.tensor([output.tolist()]), Y)\n",
    "                    loss = criterion(output, Y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "    def data_reader(self, file_name, column_name):\n",
    "        self.column_name = column_name\n",
    "        self.data = pd.read_csv(file_name)  \n",
    "        self.X = torch.tensor(self.data[self.column_name].values[:self.data_size:self.step], dtype=torch.float32).view(-1, 1)\n",
    "        # self.x = pd.read_csv('ETTh1.csv').HUFL\n",
    "        return self.data\n",
    "        \n",
    "    def set_data_function(self, func): \n",
    "        \"\"\" This methos set data by function in func\n",
    "\n",
    "        Args:\n",
    "            func(x): link to function which return y(x)\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor([func(i) for i in range(self.data_size)], dtype=torch.float32).view(-1, 1)\n",
    "    def set_data(self, df):\n",
    "        self.data = df\n",
    "        self.X = torch.tensor(self.data.values[:self.data_size:self.step], dtype=torch.float32).view(-1, 1)\n",
    "    def set_model(self, type = \"ma\"):\n",
    "        \"\"\"This method set model.\n",
    "        type:\n",
    "            \"stl\": DLinear winth STL decomposition,\n",
    "            \"ma\":  DLinear winth MA decomposition,\n",
    "            \"one_layer\": One Layer model.\n",
    "\n",
    "        Args:\n",
    "            stl (bool, optional): set model type. Defaults to false.\n",
    "        \"\"\"\n",
    "        def ma_model():\n",
    "            self.model = DLinearModel(self.input_size, self.output_size)\n",
    "            self.model_name += \"MA\"\n",
    "        def stl_model():\n",
    "            self.model = DLinearModelSTL(self.input_size, self.output_size)\n",
    "            self.model_name += \"STL\"\n",
    "        def oneLayer_model():\n",
    "            self.model = OneLayerModel(self.input_size, self.output_size)\n",
    "            self.model_name += \"oneLayer\"\n",
    "\n",
    "        setting = {\n",
    "            \"stl\": stl_model,\n",
    "            \"ma\": ma_model,\n",
    "            \"one_layer\": oneLayer_model\n",
    "        }\n",
    "        setting[type]()\n",
    "            \n",
    "    def train(self, num_epochs = 100, gpu=False):\n",
    "        # if gpu:\n",
    "        #     optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        #     self.model = self.model.to(\"xpu\")\n",
    "        #     self.model, optimizer = ipex.optimize(self.model, optimizer=optimizer, dtype=torch.float32)\n",
    "        # else:\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.L1Loss()\n",
    "        window_size = self.input_size  \n",
    "        dataset = MyDataset(self.X, window_size, self.output_size)\n",
    "        print(f\"Len Dataset = {len(dataset)}\")\n",
    "        dataloader = DataLoader(dataset)#, shuffle=True)\n",
    "        len(dataloader)\n",
    "        self.train_model(self.model, dataloader, criterion, optimizer, num_epochs=num_epochs)\n",
    "        torch.save(self.model.state_dict(), self.model_name)\n",
    "        print(f\"Model save as {self.model_name}\")\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "    \n",
    "\n",
    "    def load_modal(self, name):\n",
    "        self.model_name = name\n",
    "        self.model.load_state_dict(torch.load(f\"{self.model_name}\"))\n",
    "        self.model.eval()\n",
    "        self.model.parameters\n",
    "        \n",
    "        \n",
    "    def train__with_metrics(self, num_epochs = 1000, data_set = 3000):\n",
    "        window_size = self.input_size  \n",
    "        dataset = MyDataset(self.X, window_size, self.output_size)\n",
    "        dataloader = DataLoader(dataset)#, shuffle=True)\n",
    "        criterion = nn.L1Loss()\n",
    "    \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', cooldown = 10)\n",
    "        # lambda_1 = lambda epoch: 0.65 ** epoch\n",
    "        #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "        #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 40], gamma=0.1)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-10, last_epoch=-1)\n",
    "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=len(dataloader)*10, gamma=0.9)\n",
    "        \n",
    "        \n",
    "        print(\"Windows: \", len(dataloader))\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch = \", epoch)\n",
    "            print(f\"LR = {optimizer.param_groups[0]['lr']}\")\n",
    "            for X, Y in dataloader:\n",
    "                if(epoch <= 0):\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    #print(X, Y)\n",
    "                    output = self.model.forward(X).view(1, -1, 1)\n",
    "                    #print(torch.tensor([output.tolist()]), Y)\n",
    "                    loss = criterion(output, Y)\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                else:\n",
    "                    optimizer.zero_grad()\n",
    "                    #print(X, Y)\n",
    "                    output = self.model.forward(X).view(1, -1, 1)\n",
    "                    #print(torch.tensor([output.tolist()]), Y)\n",
    "                    loss = criterion(output, Y)\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    scheduler.step(loss)\n",
    "                \n",
    "            # result = self.prediction(data_set=data_set)\n",
    "            self.MAPE(data_set=data_set)\n",
    "        \n",
    "    \n",
    "    def decomposition(self, data_set = 3000):\n",
    "       \n",
    "        X = torch.tensor([self.data[self.column_name].values[data_set:data_set+self.input_size]], dtype=torch.float32).view(1, -1, 1)\n",
    "        # self.x = pd.read_csv('ETTh1.csv').HUFL\n",
    "        \n",
    "        print(X)\n",
    "        \n",
    "            \n",
    "            #print(X, Y)\n",
    "        trend, seasonal = self.model.decompos(X)\n",
    "        print(trend, seasonal)\n",
    "        summa = trend+seasonal\n",
    "        return trend.reshape(1, 1, -1).tolist()[0][0], seasonal.reshape(1, 1, -1).tolist()[0][0], summa.reshape(1, 1, -1).tolist()[0][0]\n",
    "            #print(torch.tensor([output.tolist()]), Y)\n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "            # result = self.prediction(data_set=data_set)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def prediction(self, data_set):\n",
    "        pred = self.input_size\n",
    "        X_f = torch.tensor(self.data[self.column_name].values[data_set-pred:data_set-pred+pred*self.step:self.step], dtype=torch.float32).view(-1, 1)\n",
    "        #print(X_f)\n",
    "        X_t = X_f.tolist()\n",
    "        # predicted_values = []\n",
    "        X = torch.tensor([X_t])\n",
    "        prediction = self.model(X)\n",
    "        #print(prediction)\n",
    "        self.predicted_values = prediction.tolist()[-1][-1]\n",
    "        return self.predicted_values\n",
    "    def prediction_custom_data(self, func):\n",
    "        pred = self.input_size\n",
    "        X_f = torch.tensor([func(i) for i in range(self.data_set-pred, self.data_set-pred+pred*self.step,self.step)], dtype=torch.float32).view(-1, 1)\n",
    "        #print(X_f)\n",
    "        X_t = X_f.tolist()\n",
    "        # predicted_values = []\n",
    "        X = torch.tensor([X_t])\n",
    "        prediction = self.model(X)\n",
    "        #print(prediction)\n",
    "        self.predicted_values = prediction.tolist()[-1][-1]\n",
    "        return self.predicted_values\n",
    "    def MAE(self, data_set):\n",
    "        i = [self.data_set+1+i*self.step for i in range(self.output_size)]\n",
    "\n",
    "        actual = np.array([self.data[self.column_name].values[k] for k in i])\n",
    "        prediction = self.prediction(data_set=data_set)\n",
    "\n",
    "        l1_loss = abs(actual - prediction)\n",
    "        mae_cost = l1_loss.mean()\n",
    "        print(\"MAE:\", mae_cost)\n",
    "        \n",
    "    def MAPE(self, data_set):\n",
    "        i = [data_set+1+i*self.step for i in range(self.output_size)]\n",
    "\n",
    "        actual = np.array([self.data[self.column_name].values[k] for k in i])\n",
    "        prediction = self.prediction(data_set=data_set)\n",
    "\n",
    "        l1_loss = abs(actual+1 - prediction-1)/abs(actual+1)\n",
    "\n",
    "        \"\"\"\n",
    "        Output:\n",
    "        [0 1 2 2]\n",
    "        \"\"\"\n",
    "\n",
    "        mape_cost = np.sum(l1_loss)/self.output_size\n",
    "        print(\"MAPE:\", mape_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return 0.01*np.sin(x/10)#1.3*x+10#np.sin(x)/100\n",
    "data_set = 14100\n",
    "input_size = 100\n",
    "output_size = 100\n",
    "learning_rate = 0.001\n",
    "step = 1\n",
    "data_size = 14000\n",
    "column_name = 'value'\n",
    "dataset_name = 'dataset_1'\n",
    "type = \"ma\"\n",
    "dLinear = DLinear(data_set, input_size, output_size, step = 1, data_size = data_size, column_name=column_name, dataset_name = dataset_name)\n",
    "data = dLinear.data_reader(file_name=dataset_name +'.csv', column_name=column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #data = dLinear.set_data(func=func)\n",
    "    dLinear.set_model(type=type)\n",
    "    #dLinear.load_modal(\"dlinear(2024-02-23_16-54-53-039394)_dataset_1_value_input100_output100MA\")\n",
    "    # dLinear.train__with_metrics(data_set=data_set, num_epochs=1000)\n",
    "    dLinear.train(num_epochs =  50, gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## solve delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_horisontal_line_mae(predict, real)-> int:\n",
    "    res = 0\n",
    "    for i in predict:\n",
    "        res += np.abs(i-real)\n",
    "    print(f\"Delta MAE: {res/len(predict)}\")\n",
    "    return res/len(predict)\n",
    "\n",
    "def delta_horisontal_line_mape(predict, real)-> int:\n",
    "    res = 0\n",
    "    for i in predict:\n",
    "        res += np.abs((i-real)/real)\n",
    "    print(f\"Delta MAPE: {res/len(predict)}\")\n",
    "    return res/len(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test1():\n",
    "    future_predictions = dLinear.prediction(data_set)\n",
    "    print(\"Future Predictions:\", future_predictions)\n",
    "    time = [data_set-output_size*step+i*step for i in range(2*output_size)]\n",
    "    print(output_size)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(time, data[column_name].values[data_set-output_size*step:data_set+(output_size)*step:step])\n",
    "    #plt.plot(, ) \n",
    "    # pred = data[column_name].values[data_set-1]\n",
    "    time = [data_set+1+i*step for i in range(output_size)]\n",
    "\n",
    "        \n",
    "    plt.plot(time, future_predictions[::], 'r--')\n",
    "    #plt.title(model_name)\n",
    "    plt.xlabel('Временные точки', fontsize=14)\n",
    "    plt.ylabel(column_name, fontsize=14)\n",
    "    dLinear.MAE(data_set=data_set)\n",
    "    dLinear.MAPE(data_set=data_set)\n",
    "    delta_horisontal_line_mae(future_predictions, data[column_name].values[data_set])\n",
    "    delta_horisontal_line_mape(future_predictions, data[column_name].values[data_set])\n",
    "    plt.ylim([data[column_name].values[data_set]-10,data[column_name].values[data_set]+10])\n",
    "    plt.savefig(f'results/rw_results/test_{datetime.datetime.now().date()}_{datetime.datetime.now().hour}_{datetime.datetime.now().minute}_{dataset_name}_{column_name}_{data_set}_{type}')\n",
    "    # plt.show()\n",
    "    #plt.savefig(model_name+\"2\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2():\n",
    "    future_predictions = dLinear.prediction_custom_data(func)\n",
    "    print(\"Future Predictions:\", future_predictions)\n",
    "    time = [data_set-output_size*step+i*step for i in range(2*output_size)]\n",
    "    print(output_size)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(time, [func(data_set-output_size*step+i*step) for i in range(2*output_size)])\n",
    "    #plt.plot(, )\n",
    "\n",
    "\n",
    "    time = [data_set+i*step for i in range(output_size)]\n",
    "    plt.plot(time, future_predictions[::], 'r--')\n",
    "    #plt.title(model_name)\n",
    "    plt.xlabel('Временные точки', fontsize=14)\n",
    "    plt.ylabel('HUFL', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def test3():\n",
    "    future_predictions = dLinear.prediction(data_set)\n",
    "    print(\"Future Predictions:\", future_predictions)\n",
    "    time = [data_set-output_size*step+i*step for i in range(2*output_size)]\n",
    "    print(output_size)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(time, data[column_name].values[data_set-output_size*step:data_set+(output_size)*step:step])\n",
    "    #plt.plot(, )\n",
    "    # pred = data[column_name].values[data_set-1]\n",
    "    time = [data_set+1+i*step for i in range(output_size)]\n",
    "\n",
    "        \n",
    "    plt.plot(time, future_predictions[::], 'r--')\n",
    "    #plt.title(model_name)\n",
    "    plt.xlabel('Временные точки', fontsize=14)\n",
    "    plt.ylabel('HUFL', fontsize=14)\n",
    "    plt.show()\n",
    "    #plt.savefig(model_name+\"2\", dpi=1000)\n",
    "    \n",
    "    dLinear.MAE()\n",
    "    dLinear.MAPE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def test_decomposition():\n",
    "    data_set =3000\n",
    "    trend, season, summa = dLinear.decomposition(data_set)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    time = [i for i in range(output_size)]\n",
    "    plt.plot(time, data[column_name].values[data_set:data_set+(output_size)])\n",
    "    print()\n",
    "    plt.plot(time, trend, 'g-.')\n",
    "        \n",
    "    plt.plot(time, season, 'g--')\n",
    "    plt.plot(time, summa, 'r--')\n",
    "    test_data = data[column_name].values[data_set:data_set+(output_size)].tolist()\n",
    "    \n",
    "    test_data = pd.Series(\n",
    "    test_data)#, index = pd.interval_range(start=data_set, end= data_set+len(test_data), periods=len(test_data))\n",
    "    \n",
    "    print(test_data)\n",
    "    print(test_data.describe())\n",
    "    stl = STL(test_data, seasonal=13, period=10)\n",
    "    \n",
    "    res = stl.fit()\n",
    "    # res.plot()\n",
    "    seas = res.seasonal\n",
    "    print(seas.head)\n",
    "    plt.plot(time, seas, 'b--')\n",
    "    plt.plot(time, res.trend, 'b-.')\n",
    "    plt.plot(time, res.trend + seas + res.resid, 'p-.')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(\n",
    "    df_size = 1000, start_value=0, threshold=0.5, \n",
    "    step_size=1, min_value=-np.inf, max_value=np.inf\n",
    "):\n",
    "    df = pd.DataFrame(index = [i for i in range(df_size)])\n",
    "    previous_value = start_value\n",
    "    for index, row in df.iterrows():\n",
    "        if previous_value < min_value:\n",
    "            previous_value = min_value\n",
    "        if previous_value > max_value:\n",
    "            previous_value = max_value\n",
    "        probability = random.random()\n",
    "        if probability >= threshold:\n",
    "            df.loc[index, 'value'] = previous_value + step_size\n",
    "        else:\n",
    "            df.loc[index, 'value'] = previous_value - step_size\n",
    "        previous_value = df.loc[index, 'value']\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_rw_ts(name: str, graph: bool, step_size: int):\n",
    "\n",
    "    res = random_walk(df_size=20000, step_size=step_size, threshold=0.5, start_value=10)\n",
    "    res.to_csv(path_or_buf=name+\".csv\")\n",
    "    \n",
    "    \n",
    "    if graph:\n",
    "        plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "        plt.rcParams.update({'font.size': 14})\n",
    "        plt.plot(res, 'g')\n",
    "        plt.savefig(\"dataset_view\")\n",
    "\n",
    "\n",
    "def plot(file_name, column_name):\n",
    "    X = pd.read_csv(file_name).value\n",
    "    # X = torch.tensor(self.data[self.column_name].values[:self.data_size:self.step], dtype=torch.float32).view(-1, 1)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(X[:11000], 'g')\n",
    "    plt.grid(visible=True)\n",
    "    plt.xlabel('Временные точки', fontsize=14)\n",
    "    plt.ylabel(column_name, fontsize=14)\n",
    "    plt.savefig(\"dataset_view\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: str, test_preferences: dict, rw_range: list, dataset_generation: bool, train_preferences: dict):\n",
    "    \"\"\"About train_model\n",
    "\n",
    "    Args:\n",
    "        model (str): type of model. Example: \"ma\", \"stl\", \"base\";\n",
    "        set_data (int): set data to test;\n",
    "        size_data (int): set data to learning;\n",
    "        rw_range (list): range to create new dataset for tests.\n",
    "    \"\"\"\n",
    "    if dataset_generation:\n",
    "\n",
    "        for i in range(rw_range[0], rw_range[1]):\n",
    "            \"\"\" \n",
    "                Generate/create new test dataset\n",
    "            \"\"\"\n",
    "            create_rw_ts(name=f\"test_ds_({i/10})\", graph=false, step_size=i/10)\n",
    "    if train_preferences:\n",
    "        data_set = train_preferences[\"set_data\"]\n",
    "        input_size = train_preferences[\"input_size\"]\n",
    "        output_size = train_preferences[\"output_size\"]\n",
    "        learning_rate = train_preferences[\"learning_rate\"]\n",
    "        step = train_preferences[\"step\"]\n",
    "        data_size = train_preferences[\"data_size\"]\n",
    "        column_name = train_preferences[\"column_name\"]\n",
    "        dataset_name = 'dataset_1'\n",
    "        type = train_preferences[\"model_type\"]\n",
    "        dLinear = DLinear(data_set, input_size, output_size, step = 1, data_size = data_size, column_name=column_name, dataset_name = dataset_name)\n",
    "        data = dLinear.data_reader(file_name=dataset_name +'.csv', column_name=column_name)\n",
    "        #data = dLinear.set_data(func=func)\n",
    "        dLinear.set_model(type=type)\n",
    "        dLinear.load_modal(\"dlinear(2024-02-23_16-54-53-039394)_dataset_1_value_input100_output100MA\")\n",
    "        # dLinear.train__with_metrics(data_set=data_set, num_epochs=1000)\n",
    "        # dLinear.train(num_epochs =  1000, gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-01 15:03:19.516380\n",
      "Len Dataset = 13800\n",
      "|████████████████████████████████████████| 50/50 [100%] in 14:00.4 (0.06/s) \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File dlinear(2024-03-01 15:03:19.398896)_dataset_1_value_input100_output100MA cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test1()\n",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m dLinear\u001b[38;5;241m.\u001b[39mset_model(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#dLinear.load_modal(\"dlinear(2024-02-23_16-54-53-039394)_dataset_1_value_input100_output100MA\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# dLinear.train__with_metrics(data_set=data_set, num_epochs=1000)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdLinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 209\u001b[0m, in \u001b[0;36mDLinear.train\u001b[1;34m(self, num_epochs, gpu)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, dataloader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n\u001b[1;32m--> 209\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel save as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n",
      "File \u001b[1;32mc:\\Users\\evgen\\miniconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\evgen\\miniconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evgen\\miniconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File dlinear(2024-03-01 15:03:19.398896)_dataset_1_value_input100_output100MA cannot be opened."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train()\n",
    "test1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
